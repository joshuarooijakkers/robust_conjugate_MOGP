{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1c3652",
   "metadata": {},
   "source": [
    "# Import packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ff278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\joshu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\joshu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\joshu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joshu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gpflow\\versions.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "from gpflow.optimizers import Scipy\n",
    "\n",
    "from rcgp.morcgp import MOGPRegressor, MORCGPRegressor, MOGPRegressor_NC, MORCGPRegressor_NC, MORCGPRegressor_NC_fixed_weights, MORCGPRegressor_fixed_weights, MORCGPRegressor_PM\n",
    "from rcgp.rcgp import RCGPRegressor\n",
    "from rcgp.kernels import ConstantMean, RBFKernel, SineMean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import MinCovDet\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,         \n",
    "    \"font.family\": \"serif\",       \n",
    "    \"text.latex.preamble\": r\"\\usepackage{amsmath}\",\n",
    "    'font.size': 28,         \n",
    "    'axes.labelsize': 28,    \n",
    "    'axes.titlesize': 30,      # <-- Add this line for title size\n",
    "    'xtick.labelsize': 24,   \n",
    "    'ytick.labelsize': 24,  \n",
    "    'legend.fontsize': 24,\n",
    "    'lines.linewidth': 5,    \n",
    "    'lines.markersize': 6   \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd95837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_A(d, r=1, base_strength=1.0, noise_level=0.1, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    shared_component = base_strength * np.ones((d, r))\n",
    "    noise = noise_level * np.random.randn(d, r)\n",
    "    A = shared_component + noise\n",
    "    return A\n",
    "\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    errors = y_true - y_pred\n",
    "    squared_errors = errors ** 2\n",
    "    mse = np.mean(squared_errors)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def nlpd(Y_true, mu_pred, var_pred):\n",
    "    epsilon = 1e-10\n",
    "    var_pred = np.maximum(var_pred, epsilon)\n",
    "    \n",
    "    nlpd_values = 0.5 * np.log(2 * np.pi * var_pred) + ((Y_true - mu_pred) ** 2) / (2 * var_pred)\n",
    "    \n",
    "    return np.mean(nlpd_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e9f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_outliers_c1(Y: np.ndarray, percent_outliers: float, start: float, end: float) -> np.ndarray:\n",
    "    if not (0 <= percent_outliers <= 1):\n",
    "        raise ValueError(\"percent_outliers must be between 0 and 1.\")\n",
    "    if start < 0 or end <= start:\n",
    "        raise ValueError(\"Invalid range: ensure 0 <= start < end.\")\n",
    "\n",
    "    Y_outliers = Y.copy()\n",
    "    N, D = Y.shape\n",
    "    total_elements = N \n",
    "    num_outliers = int(np.round(percent_outliers * total_elements))\n",
    "\n",
    "    row_indices = np.random.choice(N, num_outliers, replace=False)\n",
    "    col_indices = np.zeros(num_outliers, dtype=int) \n",
    "\n",
    "    signs = np.random.choice([-1, 1], size=num_outliers)\n",
    "\n",
    "    uniform_values = np.random.uniform(start, end, size=num_outliers) * signs\n",
    "\n",
    "    Y_outliers[row_indices, col_indices] += uniform_values\n",
    "\n",
    "    return Y_outliers\n",
    "\n",
    "def asymmetric_outliers_c1(Y: np.ndarray, percent_outliers: float, start: float, end: float) -> np.ndarray:\n",
    "    if not (0 <= percent_outliers <= 1):\n",
    "        raise ValueError(\"percent_outliers must be between 0 and 1.\")\n",
    "    if start < 0 or end <= start:\n",
    "        raise ValueError(\"Invalid range: ensure 0 <= start < end.\")\n",
    "    \n",
    "    Y_outliers = Y.copy()\n",
    "    N, D = Y.shape\n",
    "    total_elements = N \n",
    "    num_outliers = int(np.round(percent_outliers * total_elements))\n",
    "\n",
    "    row_indices = np.random.choice(N, num_outliers, replace=False)\n",
    "    col_indices = np.zeros(num_outliers, dtype=int) \n",
    "\n",
    "    uniform_values = np.random.uniform(start, end, size=num_outliers)\n",
    "\n",
    "    Y_outliers[row_indices, col_indices] += uniform_values\n",
    "\n",
    "    return Y_outliers\n",
    "\n",
    "def focused_outliers_c1(X, Y, percent_outliers, y_value):\n",
    "    X = X.copy()\n",
    "    Y = Y.copy()\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    n_outliers = int(n_samples * percent_outliers)\n",
    "\n",
    "    indices = np.random.choice(n_samples, size=n_outliers, replace=False)\n",
    "    medians = np.median(X, axis=0)\n",
    "\n",
    "    for idx in indices:\n",
    "        Y[idx, 0] = y_value\n",
    "        X[idx] = medians\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab569a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_multi(X, D=2):\n",
    "    \"\"\"\n",
    "    X: shape (N, input_dim) - multi-dimensional input\n",
    "    D: number of tasks\n",
    "    \"\"\"\n",
    "    N, input_dim = X.shape\n",
    "    X_multi = []\n",
    "    \n",
    "    for task in range(D):\n",
    "        # Add task index as last column\n",
    "        X_task = np.hstack([X, np.full((N, 1), task)])\n",
    "        X_multi.append(X_task)\n",
    "    \n",
    "    return np.vstack(X_multi)  # Shape: (N*D, input_dim + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8674a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx'\n",
    "\n",
    "# Read Excel file directly from the URL\n",
    "df = pd.read_excel(url)\n",
    "\n",
    "# Extract covariates X (columns X1 to X8)\n",
    "X = df.loc[:, 'X1':'X8'].to_numpy()\n",
    "\n",
    "# Extract target variables Y (columns Y1 and Y2)\n",
    "Y = df.loc[:, ['Y1', 'Y2']].to_numpy()\n",
    "\n",
    "empty_noise = np.array([1e-6]*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4b9d3",
   "metadata": {},
   "source": [
    "# No outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b45cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (576, 8)\n",
      "X_test shape: (192, 8)\n",
      "Y_train shape: (576, 2)\n",
      "Y_test shape: (192, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets (default test size = 25%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Y_train_scaled = scaler_Y.fit_transform(Y_train)\n",
    "Y_test_scaled = scaler_Y.transform(Y_test)\n",
    "\n",
    "print(\"X_train shape:\", X_train_scaled.shape)\n",
    "print(\"X_test shape:\", X_test_scaled.shape)\n",
    "print(\"Y_train shape:\", Y_train_scaled.shape)\n",
    "print(\"Y_test shape:\", Y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab94ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 24.3734 seconds\n"
     ]
    }
   ],
   "source": [
    "start_total = time.time()\n",
    "\n",
    "# --- 1. Prepare multi-task inputs ---\n",
    "X_multi_train = make_X_multi(X_train_scaled, D=2)\n",
    "X_multi_test = make_X_multi(X_test_scaled, D=2)\n",
    "Y_multi_train = Y_train_scaled.reshape(-1, 1, order='F')\n",
    "Y_multi_test = Y_test_scaled.reshape(-1, 1, order='F')\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]  # number of features\n",
    "D = 2  # number of tasks\n",
    "\n",
    "# --- 2. Define kernel ---\n",
    "base_kernel = gpflow.kernels.RBF(\n",
    "    lengthscales=1.0,\n",
    "    variance=1.0,\n",
    "    active_dims=list(range(input_dim))\n",
    ")\n",
    "\n",
    "coregion_kernel = gpflow.kernels.Coregion(\n",
    "    output_dim=D,\n",
    "    rank=D,\n",
    "    active_dims=[input_dim]\n",
    ")\n",
    "\n",
    "# Fix the diagonal of coregion kernel\n",
    "gpflow.utilities.set_trainable(coregion_kernel.kappa, False)\n",
    "coregion_kernel.kappa.assign(tf.ones_like(coregion_kernel.kappa) * 1e-6)\n",
    "\n",
    "# Combine kernels\n",
    "kernel = base_kernel * coregion_kernel\n",
    "\n",
    "# --- 3. Build exact GP model ---\n",
    "model_gpr = gpflow.models.GPR(\n",
    "    data=(X_multi_train, Y_multi_train),\n",
    "    kernel=kernel,\n",
    "    mean_function=None\n",
    ")\n",
    "\n",
    "# Optionally, you can fix the base kernel variance as before\n",
    "gpflow.utilities.set_trainable(base_kernel.variance, False)\n",
    "\n",
    "# --- 4. Optimize hyperparameters ---\n",
    "opt = Scipy()\n",
    "\n",
    "def objective_closure_gpr():\n",
    "    return -model_gpr.log_marginal_likelihood()\n",
    "try:\n",
    "    opt.minimize(objective_closure_gpr, model_gpr.trainable_variables, options=dict(maxiter=1000))\n",
    "except Exception as e:\n",
    "    print(f\"Optimization failed: {e}\")\n",
    "    print(\"Try reducing maxiter or checking data shapes\")\n",
    "\n",
    "# --- 5. Predict on test data ---\n",
    "mean_pred_mogp, var_pred_mogp = model_gpr.predict_y(X_multi_test)\n",
    "mu_mogp, std_mogp = mean_pred_mogp.numpy().reshape(-1, D, order='F'), np.sqrt(var_pred_mogp.numpy()).reshape(-1, D, order='F')\n",
    "\n",
    "end_total = time.time()\n",
    "print(f\"Total runtime: {end_total - start_total:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779b2393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 36.0995 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure total time\n",
    "start_total = time.time()\n",
    "\n",
    "mcd = MinCovDet(support_fraction=1.0).fit(Y_train_scaled)\n",
    "robust_covariance = mcd.covariance_\n",
    "\n",
    "initial_A = generate_A(d = 2, r = 2)\n",
    "\n",
    "morcgp = MORCGPRegressor_NC_fixed_weights(mean = 0, length_scale=1.67, noise = 0.04, A=initial_A)\n",
    "predictive_mean, predictive_variances = morcgp.fit(X_train_scaled, Y_train_scaled, B_weighted=robust_covariance, noise_weighted=1e-7)\n",
    "\n",
    "predictive_mean, predictive_variances = morcgp.optimize_loo_cv(weighted=True, print_opt_param = False, print_iter_param=False, update_weights=True)\n",
    "\n",
    "mu_morcgp, var_morcgp = morcgp.predict(X_test_scaled)\n",
    "std_morcgp = np.sqrt(var_morcgp + morcgp.noise)\n",
    "\n",
    "end_total = time.time()\n",
    "print(f\"Total runtime: {end_total - start_total:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4641a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 354.2806 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create multi-task inputs\n",
    "X_multi_train = make_X_multi(X_train_scaled, D=2)\n",
    "X_multi_test = make_X_multi(X_test_scaled, D=2)\n",
    "Y_multi_train = Y_train_scaled.reshape(-1, 1, order='F')\n",
    "Y_multi_test = Y_test_scaled.reshape(-1, 1, order='F')\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]  # This is the key fix!\n",
    "N = X_train_scaled.shape[0]\n",
    "D = 2\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "base_kernel = gpflow.kernels.RBF(\n",
    "    lengthscales=1.0, \n",
    "    variance=1.0, \n",
    "    active_dims=list(range(input_dim)) ,\n",
    ")\n",
    "\n",
    "coregion_kernel = gpflow.kernels.Coregion(\n",
    "    output_dim=D, \n",
    "    rank=D, \n",
    "    active_dims=[input_dim]  \n",
    ")\n",
    "\n",
    "gpflow.utilities.set_trainable(coregion_kernel.kappa, False)\n",
    "coregion_kernel.kappa.assign(tf.ones_like(coregion_kernel.kappa) * 1e-6)\n",
    "\n",
    "kernel = base_kernel * coregion_kernel\n",
    "\n",
    "gpflow.utilities.set_trainable(base_kernel.variance, False)\n",
    "\n",
    "likelihood_vgp = gpflow.likelihoods.StudentT(df=10.0)\n",
    "# gpflow.utilities.set_trainable(likelihood_vgp.scale, False)\n",
    "model_vgp = gpflow.models.VGP(\n",
    "    data=(X_multi_train, Y_multi_train),\n",
    "    kernel=kernel,\n",
    "    likelihood=likelihood_vgp\n",
    ")\n",
    "\n",
    "opt = Scipy()\n",
    "def objective_closure_vgp():\n",
    "    return -model_vgp.maximum_log_likelihood_objective()\n",
    "\n",
    "try:\n",
    "    opt.minimize(objective_closure_vgp, model_vgp.trainable_variables, options=dict(maxiter=1000))\n",
    "except Exception as e:\n",
    "    print(f\"Optimization failed: {e}\")\n",
    "    print(\"Try reducing maxiter or checking data shapes\")\n",
    "\n",
    "mean_pred_tmogp, var_pred_tmogp = model_vgp.predict_y(X_multi_test)\n",
    "mu_tmogp, std_tmogp = mean_pred_tmogp.numpy().reshape(-1, D, order='F'), np.sqrt(var_pred_tmogp.numpy()).reshape(-1, D, order='F')\n",
    "end_total = time.time()\n",
    "print(f\"Total runtime: {end_total - start_total:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37f5240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE MOGP: 0.13793597718016506\n",
      "RMSE MORCGP: 0.18356925293598048\n",
      "RMSE t-MOGP: 0.15728571647860648\n",
      "NLPD MOGP: -0.5644119045615371\n",
      "NLPD MORCGP: -0.263432071578953\n",
      "NLPD t-MOGP: -0.2800299288376262\n"
     ]
    }
   ],
   "source": [
    "rmse_mogp = calculate_rmse(Y_test_scaled, mu_mogp.reshape(-1, D, order='F'))\n",
    "rmse_morcgp = calculate_rmse(Y_test_scaled, mu_morcgp)\n",
    "rmse_tmogp = calculate_rmse(Y_test_scaled, mu_tmogp.reshape(-1, D, order='F'))\n",
    "\n",
    "print(\"RMSE MOGP:\", rmse_mogp)\n",
    "print(\"RMSE MORCGP:\", rmse_morcgp)\n",
    "print(\"RMSE t-MOGP:\", rmse_tmogp)\n",
    "\n",
    "nlpd_mogp = nlpd(Y_test_scaled, mu_mogp.reshape(-1, D, order='F'), std_mogp.reshape(-1, D, order='F')**2)\n",
    "nlpd_morcgp = nlpd(Y_test_scaled, mu_morcgp, std_morcgp**2)\n",
    "nlpd_tmogp = nlpd(Y_test_scaled, mu_morcgp.reshape(-1, D, order='F'), std_tmogp.reshape(-1, D, order='F')**2)\n",
    "\n",
    "print(\"NLPD MOGP:\", nlpd_mogp)\n",
    "print(\"NLPD MORCGP:\", nlpd_morcgp)\n",
    "print(\"NLPD t-MOGP:\", nlpd_tmogp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d493a",
   "metadata": {},
   "source": [
    "# Uniform outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b21f5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb66bed1",
   "metadata": {},
   "source": [
    "# Asymmetric outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "315f8145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (576, 8)\n",
      "X_test shape: (192, 8)\n",
      "Y_train shape: (576, 2)\n",
      "Y_test shape: (192, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets (default test size = 25%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Y_train_scaled = scaler_Y.fit_transform(Y_train)\n",
    "Y_test_scaled = scaler_Y.transform(Y_test)\n",
    "\n",
    "Y_train_scaled = asymmetric_outliers_c1(Y=Y_train_scaled, percent_outliers=0.1, start=6, end=12)\n",
    "\n",
    "print(\"X_train shape:\", X_train_scaled.shape)\n",
    "print(\"X_test shape:\", X_test_scaled.shape)\n",
    "print(\"Y_train shape:\", Y_train_scaled.shape)\n",
    "print(\"Y_test shape:\", Y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f41d70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 4.7374 seconds\n"
     ]
    }
   ],
   "source": [
    "start_total = time.time()\n",
    "\n",
    "# --- 1. Prepare multi-task inputs ---\n",
    "X_multi_train = make_X_multi(X_train_scaled, D=2)\n",
    "X_multi_test = make_X_multi(X_test_scaled, D=2)\n",
    "Y_multi_train = Y_train_scaled.reshape(-1, 1, order='F')\n",
    "Y_multi_test = Y_test_scaled.reshape(-1, 1, order='F')\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]  # number of features\n",
    "D = 2  # number of tasks\n",
    "\n",
    "# --- 2. Define kernel ---\n",
    "base_kernel = gpflow.kernels.RBF(\n",
    "    lengthscales=1.0,\n",
    "    variance=1.0,\n",
    "    active_dims=list(range(input_dim))\n",
    ")\n",
    "\n",
    "coregion_kernel = gpflow.kernels.Coregion(\n",
    "    output_dim=D,\n",
    "    rank=D,\n",
    "    active_dims=[input_dim]\n",
    ")\n",
    "\n",
    "# Fix the diagonal of coregion kernel\n",
    "gpflow.utilities.set_trainable(coregion_kernel.kappa, False)\n",
    "coregion_kernel.kappa.assign(tf.ones_like(coregion_kernel.kappa) * 1e-6)\n",
    "\n",
    "# Combine kernels\n",
    "kernel = base_kernel * coregion_kernel\n",
    "\n",
    "# --- 3. Build exact GP model ---\n",
    "model_gpr = gpflow.models.GPR(\n",
    "    data=(X_multi_train, Y_multi_train),\n",
    "    kernel=kernel,\n",
    "    mean_function=None\n",
    ")\n",
    "\n",
    "# Optionally, you can fix the base kernel variance as before\n",
    "gpflow.utilities.set_trainable(base_kernel.variance, False)\n",
    "\n",
    "# --- 4. Optimize hyperparameters ---\n",
    "opt = Scipy()\n",
    "\n",
    "def objective_closure_gpr():\n",
    "    return -model_gpr.log_marginal_likelihood()\n",
    "try:\n",
    "    opt.minimize(objective_closure_gpr, model_gpr.trainable_variables, options=dict(maxiter=1000))\n",
    "except Exception as e:\n",
    "    print(f\"Optimization failed: {e}\")\n",
    "    print(\"Try reducing maxiter or checking data shapes\")\n",
    "\n",
    "# --- 5. Predict on test data ---\n",
    "mean_pred_mogp, var_pred_mogp = model_gpr.predict_y(X_multi_test)\n",
    "mu_mogp, std_mogp = mean_pred_mogp.numpy().reshape(-1, D, order='F'), np.sqrt(var_pred_mogp.numpy()).reshape(-1, D, order='F')\n",
    "\n",
    "end_total = time.time()\n",
    "print(f\"Total runtime: {end_total - start_total:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d18a1491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 30.5798 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure total time\n",
    "start_total = time.time()\n",
    "\n",
    "mcd = MinCovDet(support_fraction=1.0).fit(Y_train_scaled)\n",
    "robust_covariance = mcd.covariance_\n",
    "\n",
    "initial_A = generate_A(d = 2, r = 2)\n",
    "\n",
    "morcgp = MORCGPRegressor_NC_fixed_weights(mean = 0, length_scale=1, noise = 0.05, A=initial_A)\n",
    "predictive_mean, predictive_variances = morcgp.fit(X_train_scaled, Y_train_scaled, B_weighted=robust_covariance, noise_weighted=1e-7)\n",
    "\n",
    "predictive_mean, predictive_variances = morcgp.optimize_loo_cv(weighted=True, print_opt_param = False, print_iter_param=False, update_weights=True)\n",
    "\n",
    "mu_morcgp, var_morcgp = morcgp.predict(X_test_scaled)\n",
    "std_morcgp = np.sqrt(var_morcgp + morcgp.noise)\n",
    "\n",
    "end_total = time.time()\n",
    "print(f\"Total runtime: {end_total - start_total:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16f7b99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0105718067017913 0.04816761305752576\n"
     ]
    }
   ],
   "source": [
    "print(morcgp.length_scale, morcgp.noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0f0d388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 336.1818 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create multi-task inputs\n",
    "X_multi_train = make_X_multi(X_train_scaled, D=2)\n",
    "X_multi_test = make_X_multi(X_test_scaled, D=2)\n",
    "Y_multi_train = Y_train_scaled.reshape(-1, 1, order='F')\n",
    "Y_multi_test = Y_test_scaled.reshape(-1, 1, order='F')\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]  # This is the key fix!\n",
    "N = X_train_scaled.shape[0]\n",
    "D = 2\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "base_kernel = gpflow.kernels.RBF(\n",
    "    lengthscales=1.0, \n",
    "    variance=1.0, \n",
    "    active_dims=list(range(input_dim)) ,\n",
    ")\n",
    "\n",
    "coregion_kernel = gpflow.kernels.Coregion(\n",
    "    output_dim=D, \n",
    "    rank=D, \n",
    "    active_dims=[input_dim]  \n",
    ")\n",
    "\n",
    "gpflow.utilities.set_trainable(coregion_kernel.kappa, False)\n",
    "coregion_kernel.kappa.assign(tf.ones_like(coregion_kernel.kappa) * 1e-6)\n",
    "\n",
    "kernel = base_kernel * coregion_kernel\n",
    "\n",
    "gpflow.utilities.set_trainable(base_kernel.variance, False)\n",
    "\n",
    "likelihood_vgp = gpflow.likelihoods.StudentT(df=3.0)\n",
    "# gpflow.utilities.set_trainable(likelihood_vgp.scale, False)\n",
    "model_vgp = gpflow.models.VGP(\n",
    "    data=(X_multi_train, Y_multi_train),\n",
    "    kernel=kernel,\n",
    "    likelihood=likelihood_vgp\n",
    ")\n",
    "\n",
    "opt = Scipy()\n",
    "def objective_closure_vgp():\n",
    "    return -model_vgp.maximum_log_likelihood_objective()\n",
    "\n",
    "try:\n",
    "    opt.minimize(objective_closure_vgp, model_vgp.trainable_variables, options=dict(maxiter=1000))\n",
    "except Exception as e:\n",
    "    print(f\"Optimization failed: {e}\")\n",
    "    print(\"Try reducing maxiter or checking data shapes\")\n",
    "\n",
    "mean_pred_tmogp, var_pred_tmogp = model_vgp.predict_y(X_multi_test)\n",
    "mu_tmogp, std_tmogp = mean_pred_tmogp.numpy().reshape(-1, D, order='F'), np.sqrt(var_pred_tmogp.numpy()).reshape(-1, D, order='F')\n",
    "end_total = time.time()\n",
    "print(f\"Total runtime: {end_total - start_total:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "15cf55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE MOGP: 1.006416588276257\n",
      "RMSE MORCGP: 0.1969785874010365\n",
      "RMSE t-MOGP: 0.1718626666782293\n",
      "NLPD MOGP: 1.7243622855976763\n",
      "NLPD MORCGP: 0.09863780808461464\n",
      "NLPD t-MOGP: -0.14731465560197426\n"
     ]
    }
   ],
   "source": [
    "rmse_mogp = calculate_rmse(Y_test_scaled, mu_mogp.reshape(-1, D, order='F'))\n",
    "rmse_morcgp = calculate_rmse(Y_test_scaled, mu_morcgp)\n",
    "rmse_tmogp = calculate_rmse(Y_test_scaled, mu_tmogp.reshape(-1, D, order='F'))\n",
    "\n",
    "print(\"RMSE MOGP:\", rmse_mogp)\n",
    "print(\"RMSE MORCGP:\", rmse_morcgp)\n",
    "print(\"RMSE t-MOGP:\", rmse_tmogp)\n",
    "\n",
    "nlpd_mogp = nlpd(Y_test_scaled, mu_mogp.reshape(-1, D, order='F'), std_mogp.reshape(-1, D, order='F')**2)\n",
    "nlpd_morcgp = nlpd(Y_test_scaled, mu_morcgp, std_morcgp**2)\n",
    "nlpd_tmogp = nlpd(Y_test_scaled, mu_morcgp.reshape(-1, D, order='F'), std_tmogp.reshape(-1, D, order='F')**2)\n",
    "\n",
    "print(\"NLPD MOGP:\", nlpd_mogp)\n",
    "print(\"NLPD MORCGP:\", nlpd_morcgp)\n",
    "print(\"NLPD t-MOGP:\", nlpd_tmogp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
